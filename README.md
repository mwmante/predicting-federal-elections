# predicting-federal-elections

Implemented supervised learning algorithms K-Nearest Neighbor and Perceptron in order to predict federal election outcomes using publicly available data from the Federal Election Commission sourced by required campaign filings. The dataset consisted of a sample of 1500 candidates from federal elections and information about their campaigns sourced from publicly available Federal Elections Commission data. The attributes in the subset of the data focused on the financial resources of the campaign. The continuous variables were Net Operating Expenditure, Net Contributions and Total Loans. The categorical variables were Candidate Office (P – President, S - Senate, H – House) and whether the candidate was an Incumbent, Challenger or the election was for an Open Seat. For each algorithm, the data needed to be preprocessed appropriately. Each model was trained for approximately one minute.

In order to preprocess the data for the K-Nearest Neighbor algorithm, categorical variables were encoded with a One Hot Encoding Method in which each value in the category was converted to its own column with a value of 1 if the data was in that category and 0 otherwise. Continuous variables in the dataset were normalized. In my implementation of the algorithm, I used Euclidean distance between the features and chose a k-value of 5 based on trial and error. In my own testing with an 80/20 split between testing and training data, my model was able to predict with an accuracy of almost 95%.

The Perceptron algorithm used the same preprocessing approach as the Knn algorithm. I used the sigmoid function as the activation function. If the result of the sigmoid function for the perceptron was greater than 0.5, I classified the result as an election win. Otherwise, it was classified as a loss. The perceptron also utilized a bias input and corresponding bias weight. I used a learning rate of 0.1%. The algorithm predicted with an accuracy of greater than 95% in my testing.
